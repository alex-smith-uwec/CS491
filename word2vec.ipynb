{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AZk2HS77mMl-mZyKuzIvX4dicurVJ1-p",
      "authorship_tag": "ABX9TyPNvWtm8Yg45AQx5thiKlJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-smith-uwec/CS491/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpRBWeZUE29k",
        "outputId": "f44bc652-c825-4713-980d-64c435883773"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8RXZzP-D9wx",
        "outputId": "221868b6-ae33-47cf-8bb5-1be5c5e869a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Assuming nltk is set up with necessary datasets\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_path=\"/content/drive/MyDrive/CS491/Data/US_Inaugural_Addresses\""
      ],
      "metadata": {
        "id": "LbXs0kMoFN3T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and stop words\n",
        "    tokens = [word for word in tokens if word not in string.punctuation and word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "PREuZkRZETQ-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all your text files are in the 'inauguration_speeches' directory\n",
        "directory = my_path\n",
        "all_files = os.listdir(directory)\n",
        "\n",
        "sentences = [] # This will store all sentences needed for training\n",
        "\n",
        "for filename in all_files:\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        # Preprocess and tokenize the text\n",
        "        tokens = preprocess_text(text)\n",
        "        sentences.append(tokens)"
      ],
      "metadata": {
        "id": "VWgcurmdEhjt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences[5]"
      ],
      "metadata": {
        "id": "3eo0fBkRG1G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"presidential_speeches_word2vec.model\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ROj0a1dDETTL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can use the model to find similar words, etc.\n",
        "model.wv.most_similar('freedom',topn=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkGcgGtyG7pz",
        "outputId": "6d813fb0-2ac2-41df-c129-afcc2742551c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('world', 0.9999240040779114), ('people', 0.9999042749404907)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "word1 = \"woman\"\n",
        "word2 = \"vote\"\n",
        "# word3=\"liberty\"\n",
        "\n",
        "# Get the vector for each word\n",
        "vector1 = model.wv[word1]\n",
        "vector2 = model.wv[word2]\n",
        "# vector3=model.wv[word3]\n",
        "\n",
        "# Add the vectors\n",
        "added_vector = vector1 + vector2#+vector3\n",
        "\n",
        "# You can now use the resulting vector to find similar words, for example\n",
        "similar_words = model.wv.similar_by_vector(added_vector,topn=3)\n",
        "\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5C8hzOaHAJ_",
        "outputId": "ec95dc32-2ab1-45ca-f737-8f858b5ff5df"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('vote', 0.9981316328048706), ('live', 0.9971269965171814), ('progress', 0.9969857931137085)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPD0oChKH0L-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}