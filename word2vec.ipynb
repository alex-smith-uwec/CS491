{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3hbso2-5laXO"
      ],
      "mount_file_id": "1AZk2HS77mMl-mZyKuzIvX4dicurVJ1-p",
      "authorship_tag": "ABX9TyM5vfYC4ELOy17Snj7MyfHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-smith-uwec/CS491/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpRBWeZUE29k",
        "outputId": "2e4bcb5b-e06f-4954-9f9f-e2d986915987"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8RXZzP-D9wx",
        "outputId": "68becb41-e3b8-459a-8b22-71926872cfc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_path=\"/content/drive/MyDrive/CS491/Data/US_Inaugural_Addresses\""
      ],
      "metadata": {
        "id": "LbXs0kMoFN3T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all your text files are in the 'inauguration_speeches' directory\n",
        "directory = my_path\n",
        "all_files = os.listdir(directory)\n"
      ],
      "metadata": {
        "id": "VWgcurmdEhjt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text = \"\" # This will store all text combined from all files\n",
        "\n",
        "for filename in all_files:\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        combined_text += text + \" \" # Ensure there's space between texts\n",
        "\n",
        "# Use NLTK's sent_tokenize to split the combined text into sentences"
      ],
      "metadata": {
        "id": "er6fCoUX389d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(combined_text)\n"
      ],
      "metadata": {
        "id": "wnzwF-Mf37N2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    # Tokenize each sentence into words and apply lowercasing\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    # Remove stopwords\n",
        "    words_filtered = [word for word in words if word not in stop_words and word.isalpha()]  # isalpha() helps remove punctuation\n",
        "    tokenized_sentences.append(words_filtered)\n"
      ],
      "metadata": {
        "id": "3eo0fBkRG1G_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences[50:52]"
      ],
      "metadata": {
        "id": "9vpQjqAq4wYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##How many CPU cores are availabe in Colab?\n",
        "!cat /proc/cpuinfo | grep processor | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAeEN69HKn9h",
        "outputId": "3c70a9dd-3f95-4444-9819-595fc3294b6c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokenized_sentences, vector_size=200, window=5, min_count=5, workers=4, epochs=10)\n"
      ],
      "metadata": {
        "id": "ROj0a1dDETTL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.wv.index_to_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zOqAGuzMzSG",
        "outputId": "54bdb988-682d-4fe9-e5ec-8b842806d975"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2614"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can use the model to find similar words, etc.\n",
        "model.wv.most_similar('woman',topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkGcgGtyG7pz",
        "outputId": "cb4294fa-cca7-4ff4-a025-c4aaea40e415"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('improvement', 0.9974627494812012),\n",
              " ('man', 0.9973759651184082),\n",
              " ('generation', 0.9973570108413696),\n",
              " ('every', 0.9973560571670532),\n",
              " ('rich', 0.9973542094230652),\n",
              " ('knowledge', 0.9973514676094055),\n",
              " ('mankind', 0.9973405599594116),\n",
              " ('remain', 0.9973093271255493),\n",
              " ('reach', 0.9973086714744568),\n",
              " ('find', 0.9972942471504211)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ml9DdgnT6KAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "word1 = \"liberty\"\n",
        "word2 = \"justice\"\n",
        "word3=\"peace\"\n",
        "\n",
        "# Get the vector for each word\n",
        "vector1 = model.wv[word1]\n",
        "vector2 = model.wv[word2]\n",
        "vector3=model.wv[word3]\n",
        "\n",
        "# Add the vectors\n",
        "added_vector = -vector1+vector2+vector3\n",
        "\n",
        "# You can now use the resulting vector to find similar words, for example\n",
        "# similar_words = model.wv.similar_by_vector(added_vector,topn=2)\n",
        "\n",
        "similar_words = model.wv.similar_by_vector(added_vector,topn=4)\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5C8hzOaHAJ_",
        "outputId": "e0de8b38-3682-4aad-ec26-374c1a01d880"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('justice', 0.9979832172393799), ('peace', 0.9976240992546082), ('best', 0.997306764125824), ('commerce', 0.997226357460022)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOyUMgh6-Na6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a slice of word vectors\n",
        "words = list(model.wv.index_to_key[0:])  # Adjust the slice for the number of words you want\n",
        "word_vectors = [model.wv[word] for word in words]\n"
      ],
      "metadata": {
        "id": "bPD0oChKH0L-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKMRLg4VMeT2",
        "outputId": "502a2b49-c127-40c3-f7fb-43bd5bdb4e6c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2614"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting word embedding"
      ],
      "metadata": {
        "id": "3hbso2-5laXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Convert word vectors into a 2D array of shape (number_of_words, dimensions_of_vectors)\n",
        "word_vectors_matrix = np.array(word_vectors)\n",
        "\n",
        "# Initialize and fit PCA\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_2d = pca.fit_transform(word_vectors_matrix)\n"
      ],
      "metadata": {
        "id": "u5IvsHwpLAVK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Filtering out points that have outlier coordinates, in order prevent plot scaling problems\n",
        "\n",
        "# Assuming word_vectors is a list of [x, y] points\n",
        "word_vectors = np.array(word_vectors)\n",
        "\n",
        "# Calculate Q1, Q3, and IQR for x and y coordinates\n",
        "Q1_x, Q3_x = np.percentile(word_vectors[:, 0], [25, 75])\n",
        "IQR_x = Q3_x - Q1_x\n",
        "Q1_y, Q3_y = np.percentile(word_vectors[:, 1], [25, 75])\n",
        "IQR_y = Q3_y - Q1_y\n",
        "\n",
        "# Define bounds for outliers\n",
        "lower_bound_x = Q1_x - 1.5 * IQR_x\n",
        "upper_bound_x = Q3_x + 1.5 * IQR_x\n",
        "lower_bound_y = Q1_y - 1.5 * IQR_y\n",
        "upper_bound_y = Q3_y + 1.5 * IQR_y\n",
        "\n",
        "# Filter out outliers\n",
        "filtered_vectors = [vec for vec in word_vectors if (\n",
        "    lower_bound_x <= vec[0] <= upper_bound_x and\n",
        "    lower_bound_y <= vec[1] <= upper_bound_y\n",
        ")]\n",
        "\n",
        "# If you need to filter the corresponding words as well:\n",
        "filtered_words = [word for vec, word in zip(word_vectors, words) if (\n",
        "    lower_bound_x <= vec[0] <= upper_bound_x and\n",
        "    lower_bound_y <= vec[1] <= upper_bound_y\n",
        ")]\n"
      ],
      "metadata": {
        "id": "EmCq5tpA0Hrp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Ensure filtered_vectors is a numpy array\n",
        "filtered_vectors = np.array(filtered_vectors)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Iterate over both words and vectors\n",
        "for vec, word in zip(filtered_vectors, filtered_words):\n",
        "    plt.scatter(vec[0], vec[1])\n",
        "    # plt.text(vec[0]+0.00, vec[1]+0.00, word, fontsize=9)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EEEr3qwMLBBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dO1WrfHNMFik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc to Vec"
      ],
      "metadata": {
        "id": "VpeLFGaa70DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "\n",
        "tagged_documents = []  # This will store all tagged documents\n",
        "\n",
        "def process_filename(filename):\n",
        "    # Example: Remove file extension\n",
        "    return filename.rsplit('.', 1)[0]\n",
        "\n",
        "for filename in all_files:\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        tokens = preprocess_text(text)\n",
        "        # Use the processed filename as the tag\n",
        "        tag = process_filename(filename)\n",
        "        tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))\n",
        "\n"
      ],
      "metadata": {
        "id": "DUdcz9W277Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tagged_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jFpYEGn8NXW",
        "outputId": "3040d809-1663-4838-808c-43e738a0f126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Doc2Vec model\n",
        "model = Doc2Vec(tagged_documents, vector_size=40, window=10, min_count=5, workers=4)\n",
        "model.save(\"presidential_speeches_doc2vec.model\")\n",
        "\n",
        "# After training, you can use the model to infer vector for a new document or find similar documents\n",
        "# Example: model.dv.most_similar(str(document_id))"
      ],
      "metadata": {
        "id": "jcV1fKXS8OOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_document_ids = list(model.dv.key_to_index.keys())\n",
        "sorted(list_of_document_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sum2PhYJm7x3",
        "outputId": "3701c1e4-dfeb-4695-ffab-5e44c6be48d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['01_washington_1789',\n",
              " '02_washington_1793',\n",
              " '03_adams_john_1797',\n",
              " '04_jefferson_1801',\n",
              " '05_jefferson_1805',\n",
              " '06_madison_1809',\n",
              " '07_madison_1813',\n",
              " '08_monroe_1817',\n",
              " '09_monroe_1821',\n",
              " '10_adams_john_quincy_1825',\n",
              " '11_jackson_1829',\n",
              " '12_jackson_1833',\n",
              " '13_van_buren_1837',\n",
              " '14_harrison_1841',\n",
              " '15_polk_1845',\n",
              " '16_taylor_1849',\n",
              " '17_pierce_1853',\n",
              " '18_buchanan_1857',\n",
              " '19_lincoln_1861',\n",
              " '20_lincoln_1865',\n",
              " '21_grant_1869',\n",
              " '22_grant_1873',\n",
              " '23_hayes_1877',\n",
              " '24_garfield_1881',\n",
              " '25_cleveland_1885',\n",
              " '26_harrison_1889',\n",
              " '27_cleveland_1893',\n",
              " '28_mckinley_1897',\n",
              " '29_mckinley_1901',\n",
              " '30_roosevelt_theodore_1905',\n",
              " '31_taft_1909',\n",
              " '32_wilson_1913',\n",
              " '33_wilson_1917',\n",
              " '34_harding_1921',\n",
              " '35_coolidge_1925',\n",
              " '36_hoover_1929',\n",
              " '37_roosevelt_franklin_1933',\n",
              " '38_roosevelt_franklin_1937',\n",
              " '39_roosevelt_franklin_1941',\n",
              " '40_roosevelt_franklin_1945',\n",
              " '41_truman_1949',\n",
              " '42_eisenhower_1953',\n",
              " '43_eisenhower_1957',\n",
              " '44_kennedy_1961',\n",
              " '45_johnson_1965',\n",
              " '46_nixon_1969',\n",
              " '47_nixon_1973',\n",
              " '48_carter_1977',\n",
              " '49_reagan_1981',\n",
              " '50_reagan_1985',\n",
              " '51_bush_george_h_w_1989',\n",
              " '52_clinton_1993',\n",
              " '53_clinton_1997',\n",
              " '54_bush_george_w_2001',\n",
              " '55_bush_george_w_2005',\n",
              " '56_obama_2009',\n",
              " '57_obama_2013',\n",
              " '58_trump_2017',\n",
              " '59_biden_2021']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.dv.most_similar('35_coolidge_1925',topn=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI-j2y048Ozg",
        "outputId": "02784911-b239-4975-b1ca-b6a2403066b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('23_hayes_1877', 0.9966657757759094),\n",
              " ('10_adams_john_quincy_1825', 0.9960194230079651),\n",
              " ('13_van_buren_1837', 0.9958868622779846),\n",
              " ('06_madison_1809', 0.9958724975585938),\n",
              " ('37_roosevelt_franklin_1933', 0.9956914186477661),\n",
              " ('27_cleveland_1893', 0.9956031441688538),\n",
              " ('26_harrison_1889', 0.9952513575553894),\n",
              " ('05_jefferson_1805', 0.9949816465377808),\n",
              " ('17_pierce_1853', 0.9949281215667725),\n",
              " ('11_jackson_1829', 0.9947870969772339),\n",
              " ('03_adams_john_1797', 0.9947158694267273),\n",
              " ('25_cleveland_1885', 0.9942325949668884),\n",
              " ('07_madison_1813', 0.9942099452018738),\n",
              " ('21_grant_1869', 0.9939588904380798),\n",
              " ('01_washington_1789', 0.9939163327217102),\n",
              " ('22_grant_1873', 0.9937052726745605),\n",
              " ('31_taft_1909', 0.993454098701477),\n",
              " ('16_taylor_1849', 0.9932552576065063),\n",
              " ('28_mckinley_1897', 0.9931862950325012),\n",
              " ('12_jackson_1833', 0.9925073385238647),\n",
              " ('36_hoover_1929', 0.9924031496047974),\n",
              " ('24_garfield_1881', 0.9914904832839966),\n",
              " ('29_mckinley_1901', 0.990878164768219),\n",
              " ('20_lincoln_1865', 0.9903134703636169),\n",
              " ('04_jefferson_1801', 0.990243136882782),\n",
              " ('30_roosevelt_theodore_1905', 0.9893225431442261),\n",
              " ('41_truman_1949', 0.9892436861991882),\n",
              " ('34_harding_1921', 0.9879710674285889),\n",
              " ('18_buchanan_1857', 0.9877365231513977),\n",
              " ('08_monroe_1817', 0.9861282706260681),\n",
              " ('42_eisenhower_1953', 0.9810500144958496),\n",
              " ('19_lincoln_1861', 0.9795634746551514),\n",
              " ('33_wilson_1917', 0.9785885810852051),\n",
              " ('40_roosevelt_franklin_1945', 0.9783372282981873),\n",
              " ('39_roosevelt_franklin_1941', 0.978244960308075),\n",
              " ('02_washington_1793', 0.9780291318893433),\n",
              " ('44_kennedy_1961', 0.97705078125),\n",
              " ('58_trump_2017', 0.9753729104995728),\n",
              " ('32_wilson_1913', 0.9747121930122375),\n",
              " ('56_obama_2009', 0.9725134372711182)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSw0pGdEl-Ub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}